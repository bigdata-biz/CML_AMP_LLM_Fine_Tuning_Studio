/**

CML LLM Fine Tuning Studio

This is the protobuf definition used at the API
surface of the Fine Tuning Studio (FTS) application. Given that 
this may integrate in the future as a first-class citizen of CML, 
we are prepending FTS to our objects to make it abundantly clear
that these model metadata definitions, etc., are different
from the CML model metadata definitions.

*/

syntax = "proto3";

package fine_tuning_studio;

/**
-----------------------
gRPC Service Definition
-----------------------
*/

// gRPC service representation of the Fine Tuning Studio app. This 
// presents an API surface to interact with the gRPC server for requests.
service FineTuningStudio {
  
  // Dataset operations
  rpc ListDatasets (ListDatasetsRequest) returns (ListDatasetsResponse) {}
  rpc GetDataset (GetDatasetRequest) returns (GetDatasetResponse) {}
  rpc AddDataset (AddDatasetRequest) returns (AddDatasetResponse) {}
  rpc RemoveDataset (RemoveDatasetRequest) returns (RemoveDatasetResponse) {}
  
  // Model operations
  rpc ListModels (ListModelsRequest) returns (ListModelsResponse) {}
  rpc GetModel (GetModelRequest) returns (GetModelResponse) {}
  rpc AddModel (AddModelRequest) returns (AddModelResponse) {}
  rpc ExportModel (ExportModelRequest) returns (ExportModelResponse) {}
  rpc RemoveModel (RemoveModelRequest) returns (RemoveModelResponse) {}

  // Model adapter operations
  rpc ListAdapters (ListAdaptersRequest) returns (ListAdaptersResponse) {}
  rpc GetAdapter (GetAdapterRequest) returns (GetAdapterResponse) {}
  rpc AddAdapter (AddAdapterRequest) returns (AddAdapterResponse) {}
  rpc RemoveAdapter (RemoveAdapterRequest) returns (RemoveAdapterResponse) {}
  
  // Prompt operations
  rpc ListPrompts (ListPromptsRequest) returns (ListPromptsResponse) {}
  rpc GetPrompt (GetPromptRequest) returns (GetPromptResponse) {}
  rpc AddPrompt (AddPromptRequest) returns (AddPromptResponse) {}
  rpc RemovePrompt (RemovePromptRequest) returns (RemovePromptResponse) {}

  // Training Jobs
  rpc ListFineTuningJobs (ListFineTuningJobsRequest) returns (ListFineTuningJobsResponse) {}
  rpc GetFineTuningJob (GetFineTuningJobRequest) returns (GetFineTuningJobResponse) {}
  rpc StartFineTuningJob (StartFineTuningJobRequest) returns (StartFineTuningJobResponse) {}
  rpc RemoveFineTuningJob (RemoveFineTuningJobRequest) returns (RemoveFineTuningJobResponse) {}

  // Evaluation Jobs
  rpc ListEvaluationJobs (ListEvaluationJobsRequest) returns (ListEvaluationJobsResponse) {}
  rpc GetEvaluationJob (GetEvaluationJobRequest) returns (GetEvaluationJobResponse) {}
  rpc StartEvaluationJob (StartEvaluationJobRequest) returns (StartEvaluationJobResponse) {}
  rpc RemoveEvaluationJob (RemoveEvaluationJobRequest) returns (RemoveEvaluationJobResponse) {}

  // Other general operations
  rpc GetAppState (GetAppStateRequest) returns (GetAppStateResponse) {}
}


/**
-------------------------------------------
gRPC protobuf Request/Response Definitions
-------------------------------------------
*/

// Dataset controls
message ListDatasetsRequest {

}
message ListDatasetsResponse {
  repeated DatasetMetadata datasets = 1;
}
message GetDatasetRequest {
  string id = 1;
}
message GetDatasetResponse {
  DatasetMetadata dataset = 1;
}
message AddDatasetRequest {
  // Type of dataset to be imported.
  DatasetType type = 1;

  // If this is a huggingface dataset, the huggingface name
  // should be provided.
  string huggingface_name = 2;

  // If this is a project-relative dataset, then add project
  // location
  string location = 3;
}
message AddDatasetResponse {
  DatasetMetadata dataset = 1;
}
message RemoveDatasetRequest {
  string id = 1;
}
message RemoveDatasetResponse {

}


// Model controls
message ListModelsRequest {

}
message ListModelsResponse {
  repeated ModelMetadata Models = 1;
}
message GetModelRequest {
  string id = 1;
}
message GetModelResponse {
  ModelMetadata model = 1;
}
message AddModelRequest {
  // type of model to import. This affects how the model
  // is loaded and how metadata is extracted for the model. 
  ModelType type = 1; 

  // Name of the huggingface model. This is the full huggingface
  // model name used to identify the model on HF hub.
  string huggingface_name = 2;

  // Model ID of the model in the model registry of the workspace.
  // Used when importing models from model registries.
  string model_registry_id = 3;
}
message AddModelResponse {
  ModelMetadata model = 1;
}
// Export a model out of the FTS app ecosystem. 
message ExportModelRequest {

  // Type of export model operation to perform.
  ModelType type = 1; 

  // Model ID that should be exported
  string model_id = 2;

  // Trained adapter that is to also be
  // exported (optional). Depending on the model
  // export type, any PEFT adapter weights may be
  // merged into the base model.
  string adapter_id = 3;

  // Human-friendly name to give to the exported
  // model. Might not be used if only exporting
  // model to a file output (for example, ONNX output)
  string model_name = 4;

  // Export output artifact location for export types
  // that require file-writing to project files.
  string artifact_location = 5;

  // model description for those model export
  // types that allow for descriptions.
  string model_description = 6;

}
message ExportModelResponse {

  // If a model is exported, this typically means a new
  // model is created and can technically be imported into
  // the FTS app. For this reason, the ModelMetadata object
  // can be used to store this information.
  ModelMetadata model = 1;
}
message RemoveModelRequest {
  string id = 1;
}
message RemoveModelResponse {
  
}



// Adapter controls
message ListAdaptersRequest {

}
message ListAdaptersResponse {
  repeated AdapterMetadata Adapters = 1;
}
message GetAdapterRequest {
  string id = 1;
}
message GetAdapterResponse {
  AdapterMetadata adapter = 1;
}
message AddAdapterRequest {
  // Type of Adapter to be imported.
  AdapterType type = 1;

  // If this is a huggingface Adapter, the huggingface name
  // should be provided.
  string huggingface_name = 2;

  // If this is a project-relative Adapter, then add project
  // location
  string location = 3;
}
message AddAdapterResponse {
  AdapterMetadata adapter = 1;
}
message RemoveAdapterRequest {
  string id = 1;
}
message RemoveAdapterResponse {
  
}


// Prompt controls
message ListPromptsRequest {

}
message ListPromptsResponse {
  repeated PromptMetadata Prompts = 1;
}
message GetPromptRequest {
  string id = 1;
}
message GetPromptResponse {
  PromptMetadata Prompt = 1;
}
message AddPromptRequest {
  // Type of Prompt to be imported.
  PromptType type = 1;
  
  // TODO: add prompt information

}
message AddPromptResponse {
  PromptMetadata Prompt = 1;
}
message RemovePromptRequest {
  string id = 1;
}
message RemovePromptResponse {
  
}


// FineTuningJob controls
message ListFineTuningJobsRequest {

}
message ListFineTuningJobsResponse {
  repeated FineTuningJobMetadata jobs = 1;
}
message GetFineTuningJobRequest {
  string id = 1;
}
message GetFineTuningJobResponse {
  FineTuningJobMetadata job = 1;
}
message StartFineTuningJobRequest {
  // Human-friendly identifier for the name of the output adapter.
  string adapter_name = 1;

  // The model ID of the base model that should be used as a
  // base for the fine tuning job.
  string base_model_id = 2;

  // The dataset that will be used to perform the training.
  // This dataset ID is the App-specific ID.
  string dataset_id = 3;

  // The prompt that will be used for training. This is
  // tied to the dataset for now, but that won't necessarily
  // be a many-to-one relationship in the future.
  string prompt_id = 4;

  // Number of workers to use for this fine-tuning job.
  int32 num_workers = 5;

  // Bits and bytes config used to quantize the model. If this
  // is present, then a model will be loaded with BnB config
  // enabled.
  BnbConfig bits_and_bytes_config = 6;

  // Automatically add the trained job as an adapter to the app.
  bool auto_add_adapter = 7;

  // Number of epochs to run during fine-tuning.
  int32 num_epochs = 8;

  // Learning rate to use during fine-tuning.
  float learning_rate = 9;

  // Number of CPUs to allocate for this job.
  int32 cpu = 10;

  // Number of GPUs to allocate for this job.  
  int32 gpu = 11;

  // Amount of memory to allocate for this job (e.g., '16Gi').
  int32 memory = 12;

  // Optional dataset test split to split the dataset into a training
  // dataset and an eval dataset. Evaluation datasets are used at epoch boundaries
  // during training to compute metrics and compte loss again.
  float train_test_split = 13;

  // Finetuning mramework to be used for model training.
  string finetuning_framework = 14;

  // Axolotl model training Configuration.
  AxolotlTrainConfig axolotl_train_config = 15;
}
message StartFineTuningJobResponse {
  FineTuningJobMetadata FineTuningJob = 1;
}
message RemoveFineTuningJobRequest {
  string id = 1;
}
message RemoveFineTuningJobResponse {
  
}


// EvaluationJob controls
message ListEvaluationJobsRequest {

}
message ListEvaluationJobsResponse {
  repeated EvaluationJobMetadata jobs = 1;
}
message GetEvaluationJobRequest {
  string id = 1;
}
message GetEvaluationJobResponse {
  EvaluationJobMetadata job = 1;
}
message StartEvaluationJobRequest {

  // Type of EvaluationJob to start
  EvaluationJobType type = 1;

  // The model ID of the base model that should be used as a
  // base for the job.
  string base_model_id = 2;

  // The dataset that will be used to perform the training.
  // This dataset ID is the App-specific ID.
  string dataset_id = 3;

  // Adapter ID of the adapter for this job
  string adapter_id = 4;

  // Number of CPUs to allocate for this job.
  int32 cpu = 5;

  // Number of GPUs to allocate for this job.
  int32 gpu = 6;

  // Amount of memory to allocate for this job (e.g., '16Gi').
  int32 memory = 7;
}
message StartEvaluationJobResponse {
  EvaluationJobMetadata job = 1;
}
message RemoveEvaluationJobRequest {
  string id = 1;
}
message RemoveEvaluationJobResponse {
  
}


// App state controls 
message GetAppStateRequest {
  
  // In the future, we may want to implement some form of
  // RBAC for state. But for now, as an AMP, we don't need
  // this.
  string user = 1;
}
message GetAppStateResponse {
  AppState state = 1;
}



/**
-------------------------
protobuf enum definitions
-------------------------
*/


// Type of dataset. This type determines how a 
// dataset is extracted or loaded into memory when
// running fine-tuning jobs.
//
// Note that enum values use C++ scoping rules, meaning that enum values are siblings of their type,
// not children of it.  Therefore, enum names must be unique within "fine_tuning_studio", not just within 
// their enums. Hence the prefix of the enum.
enum DatasetType {

  // Dataset is stored on HF hub and can be 
  // downloaded whenever needed.
  DATASET_TYPE_HUGGINGFACE = 0;

  // Dataset is locally stored in project files
  // and can be referenced by a dataset location.
  DATASET_TYPE_PROJECT = 1;
}

// Type of model. This AMP currently supports
// loading models in huggingface, from CML's
// model registry, and finally from local project
// files within a CML workspace.
enum ModelType {

  // Huggingface model.
  MODEL_TYPE_HUGGINGFACE = 0;

  // Model imported from project files.
  //
  // TODO: determine a way to extract model framework from the content
  // of the provided file directory (or by other parameters
  // in the model metadata request)
  MODEL_TYPE_PROJECT = 1;

  // Model was imported from CML Model Registry.
  MODEL_TYPE_MODEL_REGISTRY = 2;

}

// The model framework used for this model. Depending on
// the model type (i.e. HF, project, registry), handling
// the model may be different (for example, for local projects,
// we should specify a file/packaging format for ONNX models.)
// 
// TODO: for the most part, this AMP only supports pytorch models
// at this time. we should support other frameworks.
enum ModelFrameworkType {

  // Pytorch
  MODEL_FRAMEWORK_TYPE_PYTORCH = 0;

  // Tensorflow
  MODEL_FRAMEWORK_TYPE_TENSORFLOW = 1;

  // ONNX
  MODEL_FRAMEWORK_TYPE_ONNX = 2;

}

// Type of PEFT adapter.
enum AdapterType {

  // Project-relative PEFT adapter imported from project
  // files, probably created after a fine-tuning
  // job was ran in our app.
  ADAPTER_TYPE_PROJECT = 0;

  // Huggingface-stored adapter that can be pulled
  // down from HF hub.
  ADAPTER_TYPE_HUGGINGFACE = 1;

  // Adapter stored within the CML model registry.
  ADAPTER_TYPE_MODEL_REGISTRY = 2;
}


enum JobStatus {
  JOB_STATUS_SCHEDULED = 0;
  JOB_STATUS_RUNNING = 1;
  JOB_STATUS_SUCCESS = 2;
  JOB_STATUS_FAILURE = 3;
}

enum PromptType {
  PROMPT_TYPE_IN_PLACE = 0;
}


enum EvaluationJobType {
  EVALUATION_JOB_TYPE_MLFLOW = 0;
}

/**
-----------------------------
protobuf datatype definitions
-----------------------------
*/


// Metadata about a dataset that is being tracked in FTS. 
message DatasetMetadata {

  // FTS id of the dataset.
  string id = 1;

  // Type of the dataset.
  DatasetType type = 2;

  // human-readable name of the dataset. 
  string name = 3;
  
  // description of the dataset.
  string description = 4;

  // canonical huggingface dataset name (can be used to find
  // huggingface hub if this is a huggingface dataset)
  string huggingface_name = 5;

  // Project-relative location of the dataset that is
  // loaded into the app's state, if this is a project dataset.
  string location = 6;

  // list of features in the dataset.
  repeated string features = 7;

}


// Metadata about a registered model. This can
// apply right now to both adapters as well as
// models (TODO: need to check this logic)
message RegisteredModelMetadata {

  // Model ID of the registered model.
  string cml_registered_model_id = 1;

  // MLFlow experiment ID. This allows us to extract individual
  // model artifacts from the model registry, for example.
  string mlflow_experiment_id = 2;

  // MLFlow run ID tied to this specific model artifact. This is used
  // to extract individual model artifacts from MLFlow.
  string mlflow_run_id = 3;

}


// Metadata about a model that is loaded into the FTS
// application.
message ModelMetadata {
  
  // Global identifier for models.
  //
  // For the purpose of this
  // AMP application, during local ML model loading & inference,
  // model IDs are random unique identifiers that have no
  // significance within the CML ecosystem. Evenutally when this
  // AMP is integrated with CML model registry, we will ideally
  // be able to have a more significant model ID.
  string id = 1;

  // Type of model. This type affects the source of where models
  // are loaded from.
  ModelType type = 2;

  // framework of the model.
  ModelFrameworkType framework = 3;

  // human-friendly name for the model.
  string name = 4;

  // Name of the huggingface model. This is the human-readable
  // model name that can be used to identify a huggingface model
  // on HF hub.
  string huggingface_model_name = 5;

  // Location of the model if it is a local project model.
  string location = 6;

  // Metadata on the registered model with CML model registry,
  // if this model is a model registry type.
  RegisteredModelMetadata registered_model = 7;
}



message AdapterMetadata {

  // Unique ID of the PEFT adapter.
  string id = 1;

  // Type of model adapter.
  AdapterType type = 2;

  // Human friendly name of the adapter for tracking.
  string name = 3;

  // Corresponding model ID that this adapter is designed for. This is the
  // model ID in the FT app.
  string model_id = 4;

  // Project-relative directory where the PEFT adapter data is stored.

  // When training with HF/TRL libraries, a typical output directory
  // for PEFT adapters will contain files like:
  // * adapter_config.json
  // * adapter_model.bin
  
  // This dataclass currently just stores the location of the PEFT adapter
  // in the local directory which can then be used to load an adapter.
  string location = 5;

  // Huggingface PEFT adapter name (identifier used to find
  // the adapter on HF hub).
  string huggingface_name = 6;

  // Job ID of the job that was used to train/create this adapter. This is
  // used to determine if an adapter is completely trained or not.
  string job_id = 7;

  // Prompt ID of the prompt that was used to train this adapter.
  string prompt_id = 8;

  // Adapters should eventually have support in CML model registry. This metadata
  // will be stored here for adapters in case this is available.
  RegisteredModelMetadata registered_model = 9;
}


message PromptMetadata {

  // Unique ID of the prompt in question.
  string id = 1;

  // Human-friendly name of this prompt template
  // for use-cases elsewhere
  string name = 2;

  // ID of the dataset that uses this prompt.
  // This dataset should contain column names
  // that correspond to the items that are
  // in the list of slots.
  string dataset_id = 3;

  // Python formatted prompt string template.
  string prompt_template = 4;
}


message WorkerProps {
  int32 num_cpu = 1;
  int32 num_memory = 2;
  int32 num_gpu = 3;
}




message FineTuningJobMetadata {

  // Unique job identifier of the job. For some job implementations (local
  // fine tuning with the AMP), this job ID does not specifically have a
  // CML counterpart or significance in the CDP ecosystem.
  string job_id = 1;

  // The model ID of the base model that should be used as a
  // base for the fine tuning job.
  string base_model_id = 2;

  // The dataset that will be used to perform the training.
  // This dataset ID is the App-specific ID.
  string dataset_id = 3;

  // The prompt that will be used for training. This is
  // tied to the dataset for now, but that won't necessarily
  // be a many-to-one relationship in the future.
  string prompt_id = 4;

  // Number of workers to use for this fine-tuning job.
  int32 num_workers = 5;

  // CML identifier for the created CML job.
  string cml_job_id = 6;

  // Adapter ID of the adapter that this job is training.
  string adapter_id = 7;

  // Properties of each worker that will be spawned up.
  WorkerProps worker_props = 8;

  // Number of epochs to run during fine-tuning.
  int32 num_epochs = 9;

  // Learning rate to use during fine-tuning.
  float learning_rate = 10;

  // Output Directory in whihc adapter will be stored.
  string out_dir = 11;

  // Finetuning mramework to be used for model training.
  string finetuning_framework = 12;

  // Axolotl model training Configuration.
  AxolotlTrainConfig axolotl_train_config = 13;
}


// Bits and bytes config. Note that we need to 
// define this here purely because we serialize this
// config onto a protobuf, and the default data type in the
// transformers package is not serializable to a protobuf
// by default.
message BnbConfig {
  bool load_in_8bit = 1;
  bool load_in_4bit = 2;
  string bnb_4bit_compute_dtype = 3;
  string bnb_4bit_quant_type = 4;
  bool bnb_4bit_use_double_quant = 5;
  string bnb_4bit_quant_storage = 6;
  string quant_method = 7;
}



message EvaluationJobMetadata {

  // Unique job identifier of the job. For some job implementations (local
  // fine tuning with the AMP), this job ID does not specifically have a
  // CML counterpart or significance in the CDP ecosystem.
  string job_id = 1;

  // CML identifier for the created CML job.
  string cml_job_id = 2;

  // The model ID of the base model that should be used as a
  // base for the fine tuning job.
  string base_model_id = 3;

  // The dataset that will be used to perform the training.
  // This dataset ID is the App-specific ID.
  string dataset_id = 4;

  // Number of workers to use for this evaluation job.
  int32 num_workers = 5;

  // Adapter ID of the adapter that this job is training.
  string adapter_id = 6;

  // Properties of each worker that will be spawned up.
  WorkerProps worker_props = 7;

  // Resulting directory of evaluation
  string evaluation_dir = 8;
}

message AppState {
  repeated DatasetMetadata datasets = 1;
  repeated ModelMetadata models = 2;
  repeated FineTuningJobMetadata fine_tuning_jobs = 3;
  repeated EvaluationJobMetadata evaluation_jobs = 4;
  repeated PromptMetadata prompts = 5;
  repeated AdapterMetadata adapters = 6;
}

message AxolotlTrainConfig {
  // The base model identifier or path.
  string base_model = 1;

  // Type of model to load.
  string model_type = 2;

  // Type of tokenizer to load.
  string tokenizer_type = 3;

  // Whether to trust remote code from untrusted sources.
  bool trust_remote_code = 4;

  // Whether to use the fast version of the tokenizer.
  bool tokenizer_use_fast = 5;

  // Whether to use the legacy version of the tokenizer.
  bool tokenizer_legacy = 6;

  // Whether to use GPTQ quantization.
  bool gptq = 7;

  // Whether to load the model in 8-bit precision.
  bool load_in_8bit = 8;

  // Whether to load the model in 4-bit precision.
  bool load_in_4bit = 9;

  // Configuration for bf16 (bfloat16) precision.
  string bf16 = 10;

  // Whether to use fp16 (float16) precision.
  bool fp16 = 11;

  // Whether to use tf32 precision.
  bool tf32 = 12;

  // Datasets for finetuning the model.
  repeated DatasetConfig datasets = 13;

  // Whether to shuffle merged datasets.
  bool shuffle_merged_datasets = 14;

  // Path to the prepared dataset.
  string dataset_prepared_path = 15;

  // Size of the validation set.
  float val_set_size = 16;

  // Maximum sequence length for training.
  int32 sequence_len = 17;

  // Whether to pad inputs to the sequence length.
  bool pad_to_sequence_len = 18;

  // Whether to use sample packing.
  bool sample_packing = 19;

  // Whether to use sample packing for evaluation.
  bool eval_sample_packing = 20;

  // Adapter type for the model.
  string adapter = 21;

  // Directory for LoRA models.
  string lora_model_dir = 22;

  // Rank for LoRA.
  int32 lora_r = 23;

  // Alpha for LoRA.
  int32 lora_alpha = 24;

  // Dropout rate for LoRA.
  float lora_dropout = 25;

  // Target modules for LoRA.
  repeated string lora_target_modules = 26;

  // Whether to target all linear modules for LoRA.
  bool lora_target_linear = 27;

  // Tracking URI for MLflow.
  string mlflow_tracking_uri = 28;

  // Experiment name for MLflow.
  string mlflow_experiment_name = 29;

  // Whether to log artifacts to Hugging Face.
  bool hf_mlflow_log_artifacts = 30;

  // Directory to save the output model.
  string output_dir = 31;

  // Number of gradient accumulation steps.
  int32 gradient_accumulation_steps = 32;

  // Micro batch size for training.
  int32 micro_batch_size = 33;

  // Number of training epochs.
  int32 num_epochs = 34;

  // Ratio for warmup steps.
  float warmup_ratio = 35;

  // Learning rate for training.
  float learning_rate = 36;

  // Number of logging steps.
  int32 logging_steps = 37;

  // Number of evaluations per epoch.
  int32 evals_per_epoch = 38;

  // Whether to save models in safetensors format.
  bool save_safetensors = 39;

  // Whether to train on input sequences.
  bool train_on_inputs = 40;

  // Whether to group sequences by length.
  bool group_by_length = 41;

  // Whether to use gradient checkpointing.
  bool gradient_checkpointing = 42;

  // Scheduler for learning rate.
  string lr_scheduler = 43;

  // Optimizer for training.
  string optimizer = 44;

  // Weight decay for training.
  float weight_decay = 45;

  // Whether to enable strict mode.
  bool strict = 46;

  // Whether to use xformers attention.
  bool xformers_attention = 47;

  // Whether to use flash attention.
  bool flash_attention = 48;
}

message ListOfString {
  repeated string value = 1;
}

message DatasetConfig {
  // Path to the dataset.
  string path = 1;

  // Type of dataset.
  string type = 2;

  // Split of the dataset to train on.
  string train_on_split = 3;
}
